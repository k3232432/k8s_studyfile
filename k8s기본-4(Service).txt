Service
[(Service-1).PNG]파일 참조
pod는 자체 ip를 가지고 다른 pod와 통신할 수 있지만, 쉽게 사라지고 생성되는 특징때문에 직접통신 방식은 권장되지 않는다.

k8s는 pod와 직접통신하는 방법대신, 별도의 고정된 ip를 가진 서비스를 만들고 그 서비스를 통해 pod에 접근하는 방식이다.

Service 는 노출범위에 따라 ClusterIP, NodePort, LoadBalancer타입으로 나누어짐.



- Service(ClusterIP)만들기
ClusterIP는 클러스터 내부에 새로운 ip를 할당하고 여러개의 pod를 선택하는 로드밸런서의 기능을 함.
서비스이름을 내부 도메인서버에 등록해서 pod간에 서비스 이름으로도 통신이 가능하다.
다중 컨테이너 부분에서 만들었던 counter을 이용해서 redis를 서비스로 실행해봄
==================counter-redis-svc.yml===============
apiVersion: apps/v1
kind: Deployment
metadata:
  name: redis
spec:
  selector:
    matchLabels:
      app: counter
      tier: db
  template:
    metadata:
      labels:
        app: counter
        tier: db
    spec:
      containers:
        - name: redis
          image: redis
          ports:
            - containerPort: 6379
              protocol: TCP

---
apiVersion: v1
kind: Service
metadata:
  name: redis
spec:
  ports:
    - port: 6379
      protocol: TCP
  selector:
    app: counter
    tier: db
========================================
※하나의 ymal파일에 여러개의 리소스를 정의할때에는 "---"를 구분자로 사용
redis 생성
kubectl apply -f counter-redis-svc.yml
#pod, ReplicaSet, Deployment, Service 상태 확인
kubectl get all
redis Deployment와 Service가 생성된것을 확인 가능.

같은 클러스터에서 생성된 pod라면 redis라는 도메인으로 redisPod로 접근이 가능함.[(service-2).PNG참고]
※redis.default.svc.cluster.local로도 접근이 가능하며, 서로다른 namespace와 cluster를 구분할 수 있음.
ClusterIP 서비스 설정
정의			설명
spec.ports.port		서비스가 생성할 Port
spec.ports.targetPort	서비스가 접근할 pod의 Port(기본:Port와 동일)
spec.selector		서비스가 접근할 Pod의 label조건

redis Service의 selector는 redis Deployment에 정의한 label을 사용했기 때문에 해당 pod를 가리김
해당 pod의 6379포트로 연결테스트 진행
redis에 접근한 counter앱을 Deployment로 생성
====================counter-app.yml===========
apiVersion: apps/v1
kind: Deployment
metadata:
  name: counter
spec:
  selector:
    matchLabels:
      app: counter
      tier: app
  template:
    metadata:
      labels:
        app: counter
        tier: app
    spec:
      containers:
        - name: counter
          image: ghcr.io/subicura/counter:latest
          env:
            - name: REDIS_HOST
              value: "redis"
            - name: REDIS_PORT
              value: "6379"
=================================================
counter app pod에서 redis pod로 접근이 되는지 테스트
kubectl apply -f counter-app.yml
# counter app에 접근
kubectl get po
# counter app에 접근
kubectl exec -it counter-<podname정보 -- sh
--counter에서
app # apk add curl busybox-extras # install telnet
app # curl localhost:3000
app # curl localhost:3000
app # telnet redis 6379
--redis에서
(inDB)  dbsize
(inDB)  KEYS *
(inDB)  GET count
(inDB)  quit

- Service 생성 흐름
Service는 각 pod를 바라보는 로드밸런서 역활을 하면서 내부 도메인 서버에 새로운 도메인 서버를 생성[(service-3).PNG참조]
1. Endpoint Controller는 Service와 pod을 감시하면서 조건에 맞는 pod의 IP를 수집
2. Endpoint Controller가 수집한 IP를 가지고 Endpoint 생성
3. Kube-Proxy는 Endpoint 변화를 감시하고 노드의 iptables을 설정
4. CoreDNS는 Service를 감시하고 서비스 이름과 IP를 CoreDNS에 추가

iptalbes는 Kernel레벨의 네트워크 도우이며, CoreDNS는 빠르고 편리하게 사용할 수 있는 클러스터 내부용 DNS임
각각의 역할은 iptables 설정으로 여러 ip에 트래픽을 전달하고 CoreDNS를 이용하여 ip대신 도메인 이름을 사용함.
※ iptables : iptables는 규칙이 많아지면 성능저하가 일어나기때문에 ipvs를 사용하는 옵션도 존재
※ CoreDNS : CoreDNS는 클러스터에서 호환성을 위해 kube-dns라는 이름으로 생성됨

위의 설명에서 Endpoint가 나왔는데 
Endpoint는 서비스의 접속정보를 가지고 있음.
kubectl get endpoints
kubectl get ep
#redis Endpoint 확인
kubectl describe ep/redis
Endpoint Addresses 정보에 Redis pod의 ip를 볼 수 있음.(Replicas가 여러개라면 여러 ip확인이 가능)

- Service(NodePort)만들기
ClusterIP는 클러스터 내부에서만 접근할 수 있음. 클러스터 외부(node)에서 접근할 수 있도록 NodePort서비스 생성
======================counter-nodeport.yml================
apiVersion: v1
kind: Service
metadata:
  name: counter-np
spec:
  type: NodePort
  ports:
    - port: 3000
      protocol: TCP
      nodePort: 31000
  selector:
    app: counter
    tier: app
======================================================
정의 			설명
spec.ports.nodePort	노드에 오픈할 Port(미지정시 30000-32768중에

counter app을 해당 노드의 31000으로 오픈합니다.
kubectl apply -f counter-nodeport.yml
#서비스 상태 확인
kubectl get svc
minikube ip로 테스트 클러스터의 노드 ip를 구하고 31000으로 접근
# minikube의 ip 
minikube ip 
웹브라우저 열고, [minikube ip]:31000


NodePort는 클러스터의 모든 노드에 포트를 오픈함. [(Service-4).PNG참조]
지금은 테스트라서 하나의 노트밖에 없지만 여러개의 노드가 있다면 아무 노드로 접근해도 지정한 pod로 접근할 수 있음. [(Service-5).PNG참조]
※ NodePort는 ClusterIP의 기능을 기본으로 포함함.

- Service(LoadBalancer)
NodePort의 단점은 노드가 사라졌을 때 자동으로 다른 노드를 통해 접근이 불가능 하다는 점임.
예를 들면, 3개의 노드가 있다면 3개중에 아무 노드로 접근해도 NodePort로 연결할 수 있지만 어떤 노드가 살아 있는지는 알 수 없음.

자동으로 살아 있는 노드에 접근하기 위해 모든 노드를 바라보는 LoadBalancer가 필요함. [(Service-5).PNG참조]
브라우저는 NodePort에 직접 요청을 보내는 것이 아니라 Load Balancer에 요청하고, Load Balancer가 알아서 살아 있는 노드에 접근하면
NodePort의 단점을 매꿀수 있음.

LoadBalancer 생성
===================counter-lb.yml===================
apiVersion: v1
kind: Service
metadata:
  name: counter-lb
spec:
  type: LoadBalancer
  ports:
    - port: 30000
      targetPort: 3000
      protocol: TCP
  selector:
    app: counter
    tier: app
=================================================
#생성
kubectl apply -f counter-lb.yml
# 생성 확인
kubectl get svc

counter-lb가 생성되었지만, EXTERNAL-IP가 panding 상태인 것을 확인 할 수 있다.
LoadBalancer는 AWS,GCP,Azure 같은 클라우드 환경이 아니라면 사용이 제한적임
특정 노드를 가리키는 특정LoadBalancer가 필요한데  이런 특정LoadBalancer은 가상머신 또는 로컬서버에는 존재하지 않음.

- minikube에 가상 LoadBalancer만들기
LoadBalancer를 사용할 수 없는 환경에서 가상의 환경을 만들어 주는 MetalLB라는 게 있음. 
minikube에서는 현재 pending상태의 노드를 LoadBalancer로 설정하는데, minikube addons명령어를 통해 활성화 할 수 있음.
minkube addons enbale metallb
그런 후 minikube ip명령어로 확인한 ip를 ConfigMap으로 지정해야 함.
=================metallb-cm.yml==================
apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 172.19.66.64/32 # minikube ip
==================================================
#실행
kubectl apply -f metallb-cm.yml
#서비스 확인
kubectl get svc

172.17.66.64:30000 번으로 접근해 보면 됨.
※LoadBalancer는 기본적으로 NodePort기능을 포함함.

-마무리
서비스는 Lowlevel수준의 네트워크를 이해하고, 성능, 보안 이슈를 신경써야함.
깊이있게 갈 수록 어렵고 복잡하니까 공부를 병행하면 좋음.
실제로는 NodePort와 LoadBalancer를 제한적으로 사용함. Web을 사용하면 80포트 또는 443포트를 사용하고
하나의 포트에 여러개의 서비스를 도메인이나 경로에 따라 다르게 연결하기 때문.

-실습
문제 : echo 서비스를 NodePort로 32000포트로 오픈
키			값
Deployment이름		echo
Deployment Label		app:echo
Deployment복제수		3
Container이름		echo
Container 이미지		ghcr.io/subicura/echo:v1
NodePort이름		echo
NodePort Port		3000
NodePort NodePort	32000

정답
====================silsup7.yml===================
apiVersion: apps/v1
kind: Deployment
metadata:
  name: echo
spec:
  replicas: 3
  selector:
    matchLabels:
      app: echo
  template:
    metadata:
      labels:
        app: echo
    spec:
      containers:
        - name: echo
          image: ghcr.io/subicura/echo:v1

---
apiVersion: v1
kind: Service
metadata:
  name: echo
spec:
  type: NodePort
  ports:
    - port: 3000
      protocol: TCP
      nodePort: 32000
  selector:
    app: echo
============================================

